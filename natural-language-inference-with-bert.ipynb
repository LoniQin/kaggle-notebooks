{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lonnieqin/natural-language-inference-with-bert?scriptVersionId=113732726\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Natural Language Inference with BERT","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom transformers import BertTokenizer, TFBertModel, AutoTokenizer, TFAutoModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:11:04.918079Z","iopub.execute_input":"2022-12-13T15:11:04.918592Z","iopub.status.idle":"2022-12-13T15:11:04.924325Z","shell.execute_reply.started":"2022-12-13T15:11:04.918556Z","shell.execute_reply":"2022-12-13T15:11:04.923014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution Strategy","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\nprint('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:11:07.221648Z","iopub.execute_input":"2022-12-13T15:11:07.221976Z","iopub.status.idle":"2022-12-13T15:11:13.391466Z","shell.execute_reply.started":"2022-12-13T15:11:07.221942Z","shell.execute_reply":"2022-12-13T15:11:13.390185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    batch_size = strategy.num_replicas_in_sync * 16\n    sequence_length = 64","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:11:16.806744Z","iopub.execute_input":"2022-12-13T15:11:16.807065Z","iopub.status.idle":"2022-12-13T15:11:16.812311Z","shell.execute_reply.started":"2022-12-13T15:11:16.807026Z","shell.execute_reply":"2022-12-13T15:11:16.811064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:11:18.598259Z","iopub.execute_input":"2022-12-13T15:11:18.598558Z","iopub.status.idle":"2022-12-13T15:11:18.743294Z","shell.execute_reply.started":"2022-12-13T15:11:18.598529Z","shell.execute_reply":"2022-12-13T15:11:18.742693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Pretrained model","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    model_roBerta ='joeddav/xlm-roberta-large-xnli'\n    tokenizer = AutoTokenizer.from_pretrained(model_roBerta)\n    encoder = TFAutoModel.from_pretrained(model_roBerta)\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:11:20.991481Z","iopub.execute_input":"2022-12-13T15:11:20.992392Z","iopub.status.idle":"2022-12-13T15:13:34.505921Z","shell.execute_reply.started":"2022-12-13T15:11:20.992359Z","shell.execute_reply":"2022-12-13T15:13:34.504415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load MNLI dataset\nYou can learn more about this dataset [here](https://huggingface.co/datasets/multi_nli).","metadata":{}},{"cell_type":"code","source":"def load_mnli(use_validation=True):\n    result=[]\n    dataset=load_dataset('multi_nli')\n    print(dataset)\n    for record in dataset['train']:\n        c1, c2, c3 = record['premise'],record['hypothesis'], record['label']\n        if c1 and c2 and c3 in {0, 1, 2}:\n            result.append((c1, c2, c3, 'en'))\n    result=pd.DataFrame(result, columns=['premise', 'hypothesis', 'label', 'lang_abv'])\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:15:40.799529Z","iopub.execute_input":"2022-12-13T15:15:40.800435Z","iopub.status.idle":"2022-12-13T15:15:40.820363Z","shell.execute_reply.started":"2022-12-13T15:15:40.800398Z","shell.execute_reply":"2022-12-13T15:15:40.819287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnli = load_mnli()\nmnli","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:15:06.111639Z","iopub.execute_input":"2022-12-13T15:15:06.112585Z","iopub.status.idle":"2022-12-13T15:15:40.788466Z","shell.execute_reply.started":"2022-12-13T15:15:06.112514Z","shell.execute_reply":"2022-12-13T15:15:40.786745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.concat([train, mnli.loc[:100000]], axis=0)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:16:30.661875Z","iopub.execute_input":"2022-12-13T15:16:30.662429Z","iopub.status.idle":"2022-12-13T15:16:30.710062Z","shell.execute_reply.started":"2022-12-13T15:16:30.66236Z","shell.execute_reply":"2022-12-13T15:16:30.708435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create tensorflow dataset","metadata":{}},{"cell_type":"code","source":"def bert_encode(df, tokenizer):    \n    batch_premises = df['premise'].tolist()\n    batch_hypothesis = df['hypothesis'].tolist()\n\n    tokens = tokenizer(\n        batch_premises, \n        batch_hypothesis, \n        max_length = CFG.sequence_length,\n        truncation=True, \n        padding='max_length',\n        add_special_tokens=True, \n        return_attention_mask=True,\n        return_tensors='tf'\n    )\n    inputs = {\n        'input_ids': tokens['input_ids'], \n        'attention_mask': tokens['attention_mask']\n    }\n    if \"label\" in df.keys():\n        inputs[\"label\"] = df[\"label\"]\n    return inputs\n\ndef preprocess(features):\n    labels = features.pop(\"label\")\n    return features, labels\ndef make_dataset(df, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices((df))\n    if mode == \"train\":\n        ds = ds.shuffle(256)\n    ds = ds.batch(CFG.batch_size)\n    ds = ds.map(preprocess)\n    ds = ds.cache().prefetch(tf.data.AUTOTUNE).repeat()\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:17:02.182939Z","iopub.execute_input":"2022-12-13T15:17:02.183986Z","iopub.status.idle":"2022-12-13T15:17:02.195171Z","shell.execute_reply.started":"2022-12-13T15:17:02.183901Z","shell.execute_reply":"2022-12-13T15:17:02.194191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_data, valid_data = train_test_split(train, test_size=0.2, random_state=2)\ntrain_input = bert_encode(train_data, tokenizer)\nvalid_input = bert_encode(valid_data, tokenizer)\ntrain_ds = make_dataset(train_input)\nvalid_ds = make_dataset(valid_input, mode=\"valid\")","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:17:05.294845Z","iopub.execute_input":"2022-12-13T15:17:05.295199Z","iopub.status.idle":"2022-12-13T15:17:16.174153Z","shell.execute_reply.started":"2022-12-13T15:17:05.295161Z","shell.execute_reply":"2022-12-13T15:17:16.173185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Take a look at this dataset.","metadata":{}},{"cell_type":"code","source":"for item in train_ds.take(1):\n    print(item)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T14:43:34.289351Z","iopub.execute_input":"2022-12-13T14:43:34.289622Z","iopub.status.idle":"2022-12-13T14:43:34.328614Z","shell.execute_reply.started":"2022-12-13T14:43:34.289596Z","shell.execute_reply":"2022-12-13T14:43:34.327449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is relatively balanced. However I would like to add class_weight parameter in kera training method. This still can improve score a litte bit.","metadata":{}},{"cell_type":"code","source":"train_data[\"label\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:19:03.847398Z","iopub.execute_input":"2022-12-13T15:19:03.847935Z","iopub.status.idle":"2022-12-13T15:19:04.002192Z","shell.execute_reply.started":"2022-12-13T15:19:03.847897Z","shell.execute_reply":"2022-12-13T15:19:04.000469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weight = len(train_data[\"label\"]) / train_data[\"label\"].value_counts()\nclass_weight /= class_weight.sum()\nclass_weight = dict(class_weight)\nclass_weight","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:21:54.427153Z","iopub.execute_input":"2022-12-13T15:21:54.428159Z","iopub.status.idle":"2022-12-13T15:21:54.442398Z","shell.execute_reply.started":"2022-12-13T15:21:54.428057Z","shell.execute_reply":"2022-12-13T15:21:54.440841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calcuate class weight","metadata":{}},{"cell_type":"markdown","source":"## Building Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import regularizers\n\ndef build_model():\n    input_ids = tf.keras.Input(shape=(CFG.sequence_length,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = tf.keras.Input(shape=(CFG.sequence_length,), dtype=tf.int32, name=\"attention_mask\")\n    embedding = encoder([input_ids, attention_mask])[0]\n    vector = tf.keras.layers.GlobalAveragePooling1D()(embedding)\n    vector = tf.keras.layers.Dropout(0.3)(vector)\n    output = tf.keras.layers.Dense(3, activation='softmax')(vector)\n      \n    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])   \n    return model ","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:22:04.615388Z","iopub.execute_input":"2022-12-13T15:22:04.615715Z","iopub.status.idle":"2022-12-13T15:22:04.62498Z","shell.execute_reply.started":"2022-12-13T15:22:04.615678Z","shell.execute_reply":"2022-12-13T15:22:04.622728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n    model.summary() ","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:22:07.082868Z","iopub.execute_input":"2022-12-13T15:22:07.083425Z","iopub.status.idle":"2022-12-13T15:22:16.314483Z","shell.execute_reply.started":"2022-12-13T15:22:07.08339Z","shell.execute_reply":"2022-12-13T15:22:16.313042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps_per_epoch = train_data.shape[0] // CFG.batch_size\nvalidation_steps = valid_data.shape[0] // CFG.batch_size","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:22:19.195852Z","iopub.execute_input":"2022-12-13T15:22:19.197385Z","iopub.status.idle":"2022-12-13T15:22:19.210275Z","shell.execute_reply.started":"2022-12-13T15:22:19.197201Z","shell.execute_reply":"2022-12-13T15:22:19.20837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    es = tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_accuracy\", restore_best_weights=True)\n    history = model.fit(\n        train_ds, \n        epochs = 20, \n        steps_per_epoch = steps_per_epoch,\n        validation_steps = validation_steps,\n        validation_data=valid_ds,\n        class_weight=class_weight,\n        callbacks=[es]\n    )\n    pd.DataFrame(history.history).plot()","metadata":{"execution":{"iopub.status.busy":"2022-12-13T15:27:58.205273Z","iopub.execute_input":"2022-12-13T15:27:58.205686Z","iopub.status.idle":"2022-12-13T15:27:58.803401Z","shell.execute_reply.started":"2022-12-13T15:27:58.205643Z","shell.execute_reply":"2022-12-13T15:27:58.801711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission file","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntest_input = bert_encode(test, tokenizer)\ntest_ds = tf.data.Dataset.from_tensor_slices((test_input)).batch(CFG.batch_size)\npredictions = np.argmax(model.predict(test_ds), axis=1)\nsubmission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-13T13:32:31.712212Z","iopub.execute_input":"2022-12-13T13:32:31.712627Z","iopub.status.idle":"2022-12-13T13:32:32.336798Z","shell.execute_reply.started":"2022-12-13T13:32:31.712592Z","shell.execute_reply":"2022-12-13T13:32:32.335602Z"},"trusted":true},"execution_count":null,"outputs":[]}]}