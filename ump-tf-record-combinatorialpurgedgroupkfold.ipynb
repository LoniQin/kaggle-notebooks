{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lonnieqin/ump-tf-record-combinatorialpurgedgroupkfold?scriptVersionId=115161173\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"#  UMP TF-Record: CombinatorialPurgedGroupKFold\n\nIn this notebook, I am going to create TF-Record for UMP dataset using CombinatorialPurgedGroupKFold CV strategy.","metadata":{"papermill":{"duration":0.016475,"end_time":"2022-01-25T15:39:01.467349","exception":false,"start_time":"2022-01-25T15:39:01.450874","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport json\nimport numpy as np\nfrom scipy.special import comb\nfrom itertools import combinations\n\nclass CombinatorialPurgedGroupKFold():\n    def __init__(self, n_splits = 6, n_test_splits = 2, purge = 1, pctEmbargo = 0.01, **kwargs):\n        self.n_splits = n_splits\n        self.n_test_splits = n_test_splits\n        self.purge = purge\n        self.pctEmbargo = pctEmbargo\n        \n    def split(self, X, y = None, groups = None):\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n            \n        u, ind = np.unique(groups, return_index = True)\n        unique_groups = u[np.argsort(ind)]\n        n_groups = len(unique_groups)\n        group_dict = {}\n        for idx in range(len(X)):\n            if groups[idx] in group_dict:\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n                \n        n_folds = comb(self.n_splits, self.n_test_splits, exact = True)\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n            \n        mbrg = int(n_groups * self.pctEmbargo)\n        if mbrg < 0:\n            raise ValueError(\n                \"The number of 'embargoed' groups should not be negative\")\n        \n        split_dict = {}\n        group_test_size = n_groups // self.n_splits\n        for split in range(self.n_splits):\n            if split == self.n_splits - 1:\n                split_dict[split] = unique_groups[int(split * group_test_size):].tolist()\n            else:\n                split_dict[split] = unique_groups[int(split * group_test_size):int((split + 1) * group_test_size)].tolist()\n        \n        for test_splits in combinations(range(self.n_splits), self.n_test_splits):\n            test_groups = []\n            banned_groups = []\n            for split in test_splits:\n                test_groups += split_dict[split]\n                banned_groups += unique_groups[split_dict[split][0] - self.purge:split_dict[split][0]].tolist()\n                banned_groups += unique_groups[split_dict[split][-1] + 1:split_dict[split][-1] + self.purge + mbrg + 1].tolist()\n            train_groups = [i for i in unique_groups if (i not in banned_groups) and (i not in test_groups)]\n\n            train_idx = []\n            test_idx = []\n            for train_group in train_groups:\n                train_idx += group_dict[train_group]\n            for test_group in test_groups:\n                test_idx += group_dict[test_group]\n            yield train_idx, test_idx","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's use a small sample data to understand this CV strategy.","metadata":{}},{"cell_type":"code","source":"n_splits = 6\nn_test_splits = 2\nelements = list(range(10 * (n_splits + n_test_splits)))\ngroups = [element // n_splits for element in elements]\ndata = pd.DataFrame({\"group\": groups, \"element\": elements})\nkfold = CombinatorialPurgedGroupKFold(n_splits, n_test_splits)\nfor index, (train_indices, test_indices) in enumerate(kfold.split(data, groups=data[\"group\"])):\n    print(\"=\" * 100)\n    print(f\"Fold {index}\")\n    print(\"=\" * 100)\n    print(\"Train indices:\", train_indices, \"Length:\", len(train_indices))\n    print(\"Test Indices:\", test_indices, \"Length:\", len(test_indices))","metadata":{"execution":{"iopub.status.busy":"2022-02-25T13:52:27.008065Z","iopub.execute_input":"2022-02-25T13:52:27.008395Z","iopub.status.idle":"2022-02-25T13:52:27.036119Z","shell.execute_reply.started":"2022-02-25T13:52:27.008354Z","shell.execute_reply":"2022-02-25T13:52:27.035122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"## Import dataset","metadata":{"papermill":{"duration":0.015291,"end_time":"2022-01-25T15:39:09.296817","exception":false,"start_time":"2022-01-25T15:39:09.281526","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain.head()","metadata":{"papermill":{"duration":16.88418,"end_time":"2022-01-25T15:39:26.19638","exception":false,"start_time":"2022-01-25T15:39:09.3122","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-25T13:52:27.037512Z","iopub.execute_input":"2022-02-25T13:52:27.03845Z","iopub.status.idle":"2022-02-25T13:52:44.22674Z","shell.execute_reply.started":"2022-02-25T13:52:27.038407Z","shell.execute_reply":"2022-02-25T13:52:44.226139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id = train.pop(\"investment_id\")\ninvestment_id.head()","metadata":{"papermill":{"duration":0.041856,"end_time":"2022-01-25T15:39:26.254473","exception":false,"start_time":"2022-01-25T15:39:26.212617","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-25T13:52:44.228346Z","iopub.execute_input":"2022-02-25T13:52:44.228804Z","iopub.status.idle":"2022-02-25T13:52:44.242486Z","shell.execute_reply.started":"2022-02-25T13:52:44.228751Z","shell.execute_reply":"2022-02-25T13:52:44.241574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_id = train.pop(\"time_id\")","metadata":{"papermill":{"duration":0.045271,"end_time":"2022-01-25T15:39:26.316843","exception":false,"start_time":"2022-01-25T15:39:26.271572","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-25T13:52:46.911188Z","iopub.execute_input":"2022-02-25T13:52:46.911736Z","iopub.status.idle":"2022-02-25T13:52:46.92198Z","shell.execute_reply.started":"2022-02-25T13:52:46.911692Z","shell.execute_reply":"2022-02-25T13:52:46.921153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train.pop(\"target\")\ny.head()","metadata":{"papermill":{"duration":0.04224,"end_time":"2022-01-25T15:39:26.375506","exception":false,"start_time":"2022-01-25T15:39:26.333266","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-25T13:52:49.076678Z","iopub.execute_input":"2022-02-25T13:52:49.077671Z","iopub.status.idle":"2022-02-25T13:52:49.091191Z","shell.execute_reply.started":"2022-02-25T13:52:49.077624Z","shell.execute_reply":"2022-02-25T13:52:49.090178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create TF-Record","metadata":{"papermill":{"duration":0.017564,"end_time":"2022-01-25T15:39:30.64918","exception":false,"start_time":"2022-01-25T15:39:30.631616","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def create_record(i):\n    dic = {}\n    dic[f\"features\"] = tf.train.Feature(float_list=tf.train.FloatList(value=list(train.iloc[i])))\n    dic[\"time_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[time_id.iloc[i]]))\n    dic[\"investment_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[investment_id.iloc[i]]))\n    dic[\"target\"] = tf.train.Feature(float_list=tf.train.FloatList(value=[y.iloc[i]]))\n    record_bytes = tf.train.Example(features=tf.train.Features(feature=dic)).SerializeToString()\n    return record_bytes\n    \ndef decode_function(record_bytes):\n  return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\n          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )","metadata":{"execution":{"iopub.status.busy":"2022-02-25T13:52:51.309458Z","iopub.execute_input":"2022-02-25T13:52:51.309746Z","iopub.status.idle":"2022-02-25T13:52:51.318665Z","shell.execute_reply.started":"2022-02-25T13:52:51.309716Z","shell.execute_reply":"2022-02-25T13:52:51.317991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now create the whole dataset, it will take a long time.","metadata":{}},{"cell_type":"code","source":"%%time\nimport time\nn_splits = 5\nn_test_splits = 1\nkfold = CombinatorialPurgedGroupKFold(n_splits, n_test_splits)\nfor fold, (train_indices, test_indices) in enumerate(kfold.split(train, groups=time_id)):\n    print(\"=\" * 100)\n    print(f\"Fold {fold}\")\n    print(\"=\" * 100)\n    print(\"Train Sample size:\", len(train_indices))\n    print(\"Test Sample size:\", len(test_indices))\n    train_save_path = f\"fold_{fold}_train.tfrecords\"\n    begin = time.time()\n    print(f\"Creating {train_save_path}\")\n    with tf.io.TFRecordWriter(train_save_path) as file_writer:\n        for i in train_indices:\n            file_writer.write(create_record(i))\n    print(\"Elapsed time: %.2f\"%(time.time() - begin))\n    begin = time.time()\n    print(f\"Creating {train_save_path}\")\n    test_save_path = f\"fold_{fold}_test.tfrecords\"\n    with tf.io.TFRecordWriter(test_save_path) as file_writer:\n        for i in test_indices:\n            file_writer.write(create_record(i))\n    print(\"Elapsed time: %.2f\"%(time.time() - begin))","metadata":{"execution":{"iopub.status.busy":"2022-02-25T14:00:12.541014Z","iopub.execute_input":"2022-02-25T14:00:12.541677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Write unique Investment Ids","metadata":{}},{"cell_type":"code","source":"investment_ids = investment_id.unique()\ninvestment_id_df = pd.DataFrame({\"investment_id\": investment_ids})\ninvestment_id_df.to_csv(\"investment_ids.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}