{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lonnieqin/natural-language-inference-with-distilbert?scriptVersionId=113811132\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Natural Language Inference with DistilBert","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:47:03.942138Z","iopub.execute_input":"2022-12-14T11:47:03.942549Z","iopub.status.idle":"2022-12-14T11:47:14.808796Z","shell.execute_reply.started":"2022-12-14T11:47:03.942427Z","shell.execute_reply":"2022-12-14T11:47:14.807777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution Strategy","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\nprint('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:47:14.810473Z","iopub.execute_input":"2022-12-14T11:47:14.810738Z","iopub.status.idle":"2022-12-14T11:47:21.194645Z","shell.execute_reply.started":"2022-12-14T11:47:14.810708Z","shell.execute_reply":"2022-12-14T11:47:21.193449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    batch_size = strategy.num_replicas_in_sync * 16\n    sequence_length = 128\n    add_external_dataset = True","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:47:21.195939Z","iopub.execute_input":"2022-12-14T11:47:21.196181Z","iopub.status.idle":"2022-12-14T11:47:21.201649Z","shell.execute_reply.started":"2022-12-14T11:47:21.196153Z","shell.execute_reply":"2022-12-14T11:47:21.200163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:47:21.203438Z","iopub.execute_input":"2022-12-14T11:47:21.203691Z","iopub.status.idle":"2022-12-14T11:47:21.352092Z","shell.execute_reply.started":"2022-12-14T11:47:21.203661Z","shell.execute_reply":"2022-12-14T11:47:21.35136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Pretrained model","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n    encoder = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:47:21.353886Z","iopub.execute_input":"2022-12-14T11:47:21.354454Z","iopub.status.idle":"2022-12-14T11:47:39.522725Z","shell.execute_reply.started":"2022-12-14T11:47:21.354407Z","shell.execute_reply":"2022-12-14T11:47:39.521759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:47:39.524231Z","iopub.execute_input":"2022-12-14T11:47:39.524476Z","iopub.status.idle":"2022-12-14T11:47:39.537713Z","shell.execute_reply.started":"2022-12-14T11:47:39.524449Z","shell.execute_reply":"2022-12-14T11:47:39.536667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's learn about this pretrained model's input and output. When we past a text list to this pretrained model, it returns a dictionary with  key last_hidden_state.","metadata":{}},{"cell_type":"code","source":"texts = [\"hello world.\", \"how are you doing?\"]\ntext_preprocessed = tokenizer(\n    texts, \n    max_length = CFG.sequence_length,\n    truncation=True, \n    padding='max_length',\n    add_special_tokens=True,\n    return_tensors='tf'\n)\noutput = encoder(text_preprocessed)\noutput","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:47:39.539056Z","iopub.execute_input":"2022-12-14T11:47:39.539306Z","iopub.status.idle":"2022-12-14T11:47:39.950257Z","shell.execute_reply.started":"2022-12-14T11:47:39.539278Z","shell.execute_reply":"2022-12-14T11:47:39.948855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess_data(df):\n    df[\"text\"] = \"[CLS] \" + df[\"premise\"] + \" [SEP] \" + df[\"hypothesis\"] + \" [SEP]\"\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:47:39.952061Z","iopub.execute_input":"2022-12-14T11:47:39.95244Z","iopub.status.idle":"2022-12-14T11:47:39.957943Z","shell.execute_reply.started":"2022-12-14T11:47:39.952382Z","shell.execute_reply":"2022-12-14T11:47:39.956894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = preprocess_data(train)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:47:39.9598Z","iopub.execute_input":"2022-12-14T11:47:39.960185Z","iopub.status.idle":"2022-12-14T11:47:40.02629Z","shell.execute_reply.started":"2022-12-14T11:47:39.960141Z","shell.execute_reply":"2022-12-14T11:47:40.025263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"## Load MNLI dataset\nYou can learn more about this dataset [here](https://huggingface.co/datasets/multi_nli).","metadata":{}},{"cell_type":"code","source":"def load_mnli(use_validation=True):\n    result=[]\n    dataset=load_dataset('multi_nli')\n    print(dataset)\n    for record in dataset['train']:\n        c1, c2, c3 = record['premise'],record['hypothesis'], record['label']\n        if c1 and c2 and c3 in {0, 1, 2}:\n            result.append((c1, c2, c3, 'en'))\n    result=pd.DataFrame(result, columns=['premise', 'hypothesis', 'label', 'lang_abv'])\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:47:40.027541Z","iopub.execute_input":"2022-12-14T11:47:40.027887Z","iopub.status.idle":"2022-12-14T11:47:40.03849Z","shell.execute_reply.started":"2022-12-14T11:47:40.027842Z","shell.execute_reply":"2022-12-14T11:47:40.036745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnli = load_mnli()\nmnli.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:47:40.041648Z","iopub.execute_input":"2022-12-14T11:47:40.04252Z","iopub.status.idle":"2022-12-14T11:49:28.950814Z","shell.execute_reply.started":"2022-12-14T11:47:40.04244Z","shell.execute_reply":"2022-12-14T11:49:28.949409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnli = preprocess_data(mnli)\nmnli.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:49:28.952725Z","iopub.execute_input":"2022-12-14T11:49:28.953069Z","iopub.status.idle":"2022-12-14T11:49:29.753869Z","shell.execute_reply.started":"2022-12-14T11:49:28.953033Z","shell.execute_reply":"2022-12-14T11:49:29.752682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create TensorFlow dataset","metadata":{}},{"cell_type":"code","source":"def bert_encode(df, tokenizer):    \n    texts = df['text'].tolist()\n    tokens = tokenizer(\n        texts, \n        max_length = CFG.sequence_length,\n        truncation=True, \n        padding='max_length',\n        add_special_tokens=True,\n        return_tensors='tf'\n    )\n    if \"label\" in df.keys():\n        tokens[\"label\"] = df[\"label\"]\n    return tokens\n\ndef preprocess(features):\n    input_ids = features.pop(\"input_ids\")\n    labels = features.pop(\"label\")\n    return input_ids, labels\ndef make_dataset(df, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices((df))\n    if mode == \"train\":\n        ds = ds.shuffle(256)\n    ds = ds.batch(CFG.batch_size)\n    ds = ds.map(preprocess)\n    ds = ds.cache().prefetch(tf.data.AUTOTUNE).repeat()\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:49:29.755584Z","iopub.execute_input":"2022-12-14T11:49:29.75592Z","iopub.status.idle":"2022-12-14T11:49:29.766234Z","shell.execute_reply.started":"2022-12-14T11:49:29.755883Z","shell.execute_reply":"2022-12-14T11:49:29.764769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_data, valid_data = train_test_split(train, test_size=0.2, random_state=2)\nif CFG.add_external_dataset:\n    train_data=pd.concat([train_data, mnli.loc[:100000]], axis=0)\n    train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:49:29.769956Z","iopub.execute_input":"2022-12-14T11:49:29.770282Z","iopub.status.idle":"2022-12-14T11:49:29.834067Z","shell.execute_reply.started":"2022-12-14T11:49:29.770248Z","shell.execute_reply":"2022-12-14T11:49:29.832658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_input = bert_encode(train_data, tokenizer)\nvalid_input = bert_encode(valid_data, tokenizer)\ntrain_ds = make_dataset(train_input)\nvalid_ds = make_dataset(valid_input, mode=\"valid\")","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:49:29.836052Z","iopub.execute_input":"2022-12-14T11:49:29.83637Z","iopub.status.idle":"2022-12-14T11:51:30.349393Z","shell.execute_reply.started":"2022-12-14T11:49:29.836314Z","shell.execute_reply":"2022-12-14T11:51:30.348506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Take a look at what training data looks like.","metadata":{}},{"cell_type":"code","source":"for item in train_ds.take(1):\n    print(item)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:51:30.350805Z","iopub.execute_input":"2022-12-14T11:51:30.35156Z","iopub.status.idle":"2022-12-14T11:51:30.436445Z","shell.execute_reply.started":"2022-12-14T11:51:30.351526Z","shell.execute_reply":"2022-12-14T11:51:30.435153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calcuate class weight","metadata":{}},{"cell_type":"markdown","source":"The dataset is relatively balanced. However I would like to add class_weight parameter in keras training method. In this way we often can improve the score a little bit.","metadata":{}},{"cell_type":"code","source":"train_data[\"label\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:51:30.438246Z","iopub.execute_input":"2022-12-14T11:51:30.438843Z","iopub.status.idle":"2022-12-14T11:51:30.685982Z","shell.execute_reply.started":"2022-12-14T11:51:30.438804Z","shell.execute_reply":"2022-12-14T11:51:30.685107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weight = len(train_data[\"label\"]) / train_data[\"label\"].value_counts()\nclass_weight = dict(class_weight / class_weight.sum())\nclass_weight","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:51:30.687777Z","iopub.execute_input":"2022-12-14T11:51:30.688161Z","iopub.status.idle":"2022-12-14T11:51:30.699355Z","shell.execute_reply.started":"2022-12-14T11:51:30.688131Z","shell.execute_reply":"2022-12-14T11:51:30.69845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Model","metadata":{}},{"cell_type":"code","source":"def build_model():\n    inputs = tf.keras.Input(shape=(CFG.sequence_length,), dtype=tf.int32, name=\"input_ids\")\n    embedding = encoder(inputs)[\"last_hidden_state\"]\n    vector = tf.keras.layers.GlobalAveragePooling1D()(embedding)\n    vector = tf.keras.layers.Dropout(0.3)(vector)\n    output = tf.keras.layers.Dense(3, activation='softmax')(vector)\n      \n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])   \n    return model ","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:51:30.724129Z","iopub.execute_input":"2022-12-14T11:51:30.724401Z","iopub.status.idle":"2022-12-14T11:51:30.735167Z","shell.execute_reply.started":"2022-12-14T11:51:30.724373Z","shell.execute_reply":"2022-12-14T11:51:30.733815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n    model.summary() ","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:51:30.736395Z","iopub.execute_input":"2022-12-14T11:51:30.73665Z","iopub.status.idle":"2022-12-14T11:51:36.665334Z","shell.execute_reply.started":"2022-12-14T11:51:30.736623Z","shell.execute_reply":"2022-12-14T11:51:36.663471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:51:36.666661Z","iopub.execute_input":"2022-12-14T11:51:36.666887Z","iopub.status.idle":"2022-12-14T11:51:38.194672Z","shell.execute_reply.started":"2022-12-14T11:51:36.66686Z","shell.execute_reply":"2022-12-14T11:51:38.193703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model\nWhen I train the model using TPU and and try to save the whole model, error occurs. I solve this issue by saving weights only and save to h5 format. Training model on GPU doesn't have such issue.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    steps_per_epoch = train_data.shape[0] // CFG.batch_size\n    validation_steps = valid_data.shape[0] // CFG.batch_size\n    es = tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_accuracy\")\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        \"model.h5\", \n        monitor=\"val_accuracy\", \n        save_best_only=True, \n        save_weights_only=True, \n        restore_best_weights=True\n    )\n    history = model.fit(\n        train_ds, \n        epochs = 20, \n        steps_per_epoch = steps_per_epoch,\n        validation_steps = validation_steps,\n        validation_data=valid_ds,\n        class_weight=class_weight,\n        callbacks=[es, checkpoint]\n    )\n    pd.DataFrame(history.history).plot()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T12:02:47.189742Z","iopub.execute_input":"2022-12-14T12:02:47.190451Z","iopub.status.idle":"2022-12-14T12:02:52.767381Z","shell.execute_reply.started":"2022-12-14T12:02:47.19041Z","shell.execute_reply":"2022-12-14T12:02:52.765797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission file","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntest = preprocess_data(test)\ntest_input = bert_encode(test, tokenizer)\ntest_ds = tf.data.Dataset.from_tensor_slices((test_input[\"input_ids\"])).batch(CFG.batch_size)\npredictions = np.argmax(model.predict(test_ds), axis=1)\nsubmission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T11:53:42.772708Z","iopub.status.idle":"2022-12-14T11:53:42.773536Z","shell.execute_reply.started":"2022-12-14T11:53:42.773209Z","shell.execute_reply":"2022-12-14T11:53:42.773239Z"},"trusted":true},"execution_count":null,"outputs":[]}]}